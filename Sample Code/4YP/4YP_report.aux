\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{unsrt}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{brochu2010tutorial}
\citation{hennig2012entropy}
\citation{brochu2010tutorial}
\citation{kushner1964new}
\citation{jones1998efficient}
\citation{srinivas2009gaussian}
\citation{shahriari2016taking}
\citation{hennig2012entropy}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{brochu2010tutorial}
\citation{shahriari2016taking}
\citation{brochu2010tutorial}
\citation{brochu2010tutorial}
\citation{brochu2010tutorial}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Literature Review}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Bayesian Optimisation}{4}{section.2.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Bayesian optimisation \relax }}{4}{algorithm.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{BO}{{1}{4}{Bayesian optimisation \relax }{algorithm.1}{}}
\citation{mockus1994application}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\citation{brochu2010tutorial}
\citation{C24 Advanced probability notes}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\newlabel{BOdemonstration}{{\caption@xref {BOdemonstration}{ on input line 129}}{5}{Bayesian Optimisation}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Demonstration of the use of Bayesian optimisation on a 1D example. The figure shows the mean and confidence intervals of the Gaussian process model that approximates the objective function. The acquisition function (green) is high where the Gaussian process model predicts a high objective (exploitation) and where the prediction uncertainty is high (exploration). It is interesting to note that the region on the far left remains unsampled because while it has high uncertainty, it is correctly predicted to offer little improvement over the highest observation \cite  {brochu2010tutorial} \relax }}{5}{figure.caption.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}The Gaussian Process Prior}{5}{subsection.2.1.1}}
\citation{rasmussen2006gaussian}
\citation{brochu2010tutorial}
\citation{shahriari2016taking}
\citation{bishop2006pattern}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\newlabel{posteriorf}{{2.3}{6}{The Gaussian Process Prior}{equation.2.1.3}{}}
\newlabel{covarianceofposter}{{2.5}{6}{The Gaussian Process Prior}{equation.2.1.5}{}}
\newlabel{posteriory}{{2.6}{6}{The Gaussian Process Prior}{equation.2.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1.1}Covariance Functions}{7}{subsubsection.2.1.1.1}}
\newlabel{seard}{{\caption@xref {seard}{ on input line 197}}{7}{Covariance Functions}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Random samples generated by a Gaussian process prior with the SE-ARD covariance function and different characteristic lengths.\relax }}{7}{figure.caption.2}}
\citation{brochu2010tutorial}
\citation{shahriari2016taking}
\citation{kushner1964new}
\citation{brochu2010tutorial}
\citation{kushner1964new}
\citation{jones2001taxonomy}
\citation{dixon1978towards}
\citation{jones1998efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Acquisition Functions }{8}{subsection.2.1.2}}
\newlabel{acqusitionfunction}{{2.1.2}{8}{Acquisition Functions }{subsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.1}Improvement-based Acquisition Functions}{8}{subsubsection.2.1.2.1}}
\citation{dixon1978towards}
\citation{lizotte2008practical}
\citation{cox1992statistical}
\citation{srinivas2009gaussian}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.2}Optimistic Acquisition Functions}{9}{subsubsection.2.1.2.2}}
\newlabel{upc}{{2.13}{9}{Optimistic Acquisition Functions}{equation.2.1.13}{}}
\citation{hennig2012entropy}
\citation{hennig2012entropy}
\citation{hernandez2014predictive}
\citation{hennig2012entropy}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\newlabel{gp-upc}{{2.15}{10}{Optimistic Acquisition Functions}{equation.2.1.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2.3}Information-based Acquisition Function}{10}{subsubsection.2.1.2.3}}
\newlabel{Infoaf}{{2.1.2.3}{10}{Information-based Acquisition Function}{subsubsection.2.1.2.3}{}}
\newlabel{ESacuqisitionfunction}{{2.16}{10}{Information-based Acquisition Function}{equation.2.1.16}{}}
\newlabel{PESacqfunc}{{2.17}{10}{Information-based Acquisition Function}{equation.2.1.17}{}}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\citation{bochner2016lectures}
\citation{rahimi2007random}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Predictive Entropy Search}{11}{subsection.2.1.3}}
\newlabel{PESsection}{{2.1.3}{11}{Predictive Entropy Search}{subsection.2.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.1}Algorithm}{11}{subsubsection.2.1.3.1}}
\newlabel{PESderivation}{{2.1.3.1}{11}{Algorithm}{subsubsection.2.1.3.1}{}}
\newlabel{PESacqfunc2}{{2.18}{11}{Algorithm}{equation.2.1.18}{}}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\citation{rasmussen2006gaussian}
\newlabel{C11}{{2.22}{12}{Algorithm}{equation.2.1.22}{}}
\newlabel{C1C2}{{2.23}{12}{Algorithm}{equation.2.1.23}{}}
\citation{hernandez2014predictive}
\citation{snoek2012practical}
\citation{vanhatalo2011bayesian}
\newlabel{posteriorC1C2}{{2.25}{13}{Algorithm}{equation.2.1.25}{}}
\newlabel{C3}{{2.28}{13}{Algorithm}{equation.2.1.28}{}}
\newlabel{variancexgivenxstar}{{2.29}{13}{Algorithm}{equation.2.1.29}{}}
\newlabel{approxPES}{{2.31}{13}{Algorithm}{equation.2.1.31}{}}
\citation{requeimaintegrated}
\citation{hernandez2014predictive}
\citation{bitbucket code}
\citation{wang2017max}
\citation{wang2017max}
\citation{wang2017max}
\citation{wang2017max}
\citation{Hoffman and Ghahramani}
\citation{2015}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\newlabel{marginalisedPES}{{2.32}{14}{Algorithm}{equation.2.1.32}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces PES Method \relax }}{14}{algorithm.2}}
\newlabel{methodpes}{{2}{14}{PES Method \relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3.2}Limitations of PES}{14}{subsubsection.2.1.3.2}}
\newlabel{LimitationsofPES}{{2.1.3.2}{14}{Limitations of PES}{subsubsection.2.1.3.2}{}}
\newlabel{covariancelimitations}{{2.1.3.2}{14}{Limitations of PES}{subsubsection.2.1.3.2}{}}
\newlabel{input-spaceconstraints}{{2.1.3.2}{15}{Limitations of PES}{subsubsection.2.1.3.2}{}}
\newlabel{additionalsamplingprocess}{{2.1.3.2}{15}{Limitations of PES}{subsubsection.2.1.3.2}{}}
\newlabel{Tediousapproximationprocess}{{2.1.3.2}{16}{Limitations of PES}{subsubsection.2.1.3.2}{}}
\citation{gunter2014sampling}
\citation{gunter2014sampling}
\citation{gunter2014sampling}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Entropy Search for Bayesian Optimisation Based on Parabolic Approximation}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Parabolic Approximation and Linearisation}{17}{section.3.1}}
\newlabel{prarabolicandlinearisation}{{3.1}{17}{Parabolic Approximation and Linearisation}{section.3.1}{}}
\citation{rasmussen2006gaussian}
\citation{bishop2006pattern}
\newlabel{mf}{{3.7}{18}{Parabolic Approximation and Linearisation}{equation.3.1.7}{}}
\newlabel{Kf}{{3.8}{18}{Parabolic Approximation and Linearisation}{equation.3.1.8}{}}
\newlabel{posteriory}{{3.10}{18}{Parabolic Approximation and Linearisation}{equation.3.1.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Derivation of ESBOPA Algorithm}{18}{section.3.2}}
\citation{wang2017max}
\citation{hoffman2015}
\citation{wang2017max}
\citation{hoffman2015}
\citation{wang2017max}
\citation{bishop2006pattern}
\newlabel{acesbopa}{{3.16}{19}{Derivation of ESBOPA Algorithm}{equation.3.2.16}{}}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Entropy of a Gaussian Mixture}{20}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2} Posterior Distribution on $ \boldsymbol  {\eta }$ }{20}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Expected Entropy of a Gaussian Process}{20}{subsection.3.2.3}}
\citation{rasmussen2006gaussian}
\citation{C19 lecture notes}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\newlabel{acesbopa2}{{3.21}{21}{Expected Entropy of a Gaussian Process}{equation.3.2.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Hyperparameter Tunning}{21}{section.3.3}}
\newlabel{hyperparametertunning}{{3.3}{21}{Hyperparameter Tunning}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Maximum Likelihood Estimation}{21}{subsection.3.3.1}}
\newlabel{loglikelihood}{{3.22}{21}{Maximum Likelihood Estimation}{equation.3.3.22}{}}
\citation{bishop2006pattern}
\citation{bishop2006pattern}
\citation{rasmussen2006gaussian}
\newlabel{logml}{{\caption@xref {logml}{ on input line 593}}{22}{Maximum Likelihood Estimation}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  The log marginal likelihood function and the effect of its components\cite  {rasmussen2006gaussian} \relax }}{22}{figure.caption.3}}
\newlabel{hypposterior}{{3.24}{22}{Maximum Likelihood Estimation}{equation.3.3.24}{}}
\newlabel{hypposterior}{{3.25}{22}{Maximum Likelihood Estimation}{equation.3.3.25}{}}
\citation{hoffman2015}
\citation{snoek2012practical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Marginalisation}{23}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}ESBOPA Algorithm}{23}{section.3.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces ESBOPA Version2 \relax }}{23}{algorithm.3}}
\newlabel{ESBOPA }{{3}{23}{ESBOPA Version2 \relax }{algorithm.3}{}}
\citation{huber2008entropy}
\citation{huber2008entropy}
\citation{huber2008entropy}
\citation{huber2008entropy}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Approximation of a Gaussian Mixture Entropy}{24}{section.3.5}}
\newlabel{approxHGMM}{{3.5}{24}{Approximation of a Gaussian Mixture Entropy}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Method 1: Taylor Expansion}{24}{subsection.3.5.1}}
\newlabel{entropyofmixtureqh}{{3.28}{24}{Method 1: Taylor Expansion}{equation.3.5.28}{}}
\newlabel{taylorexpoflogh}{{3.29}{24}{Method 1: Taylor Expansion}{equation.3.5.29}{}}
\newlabel{1DESBOPA}{{\caption@xref {1DESBOPA}{ on input line 665}}{25}{ESBOPA Algorithm}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Bayesian optimisation for a 1-D objective function using ESBOPA method. In each subfigure, the top plot shows the latent objective function (red dotted line), the posterior mean (black solid line) and the 95$\%$ confidence interval (blue shaded area) estimated by the Gaussian process model as well as the observation points (black solid dot) and the next query point (red solid dot with black edge).The middle plot is the histogram of $\eta $ samples and its relation to the minimum observation (black vertical line) and the true global minimum (red vertical line). The bottom plot illustrates ESBOPA acquisition function which is maximised to select the next query point.\relax }}{25}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Method 2: Numerical Integration}{26}{subsection.3.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Method 3: Simple Monte Carlo }{26}{subsection.3.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Experiments for Comparing Approximation Methods}{27}{subsection.3.5.4}}
\newlabel{fig:fig_medi_abs_err}{{\caption@xref {fig:fig_medi_abs_err}{ on input line 776}}{28}{Experiments for Comparing Approximation Methods}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  Log median absolute error in approximating the entropy of a Gaussian mixture.\relax }}{28}{figure.caption.5}}
\newlabel{fig:fig_medi_frac_err}{{\caption@xref {fig:fig_medi_frac_err}{ on input line 784}}{28}{Experiments for Comparing Approximation Methods}{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Log median fractional error in approximating the entropy of a gaussian mixture \relax }}{28}{figure.caption.6}}
\newlabel{runningtime}{{\caption@xref {runningtime}{ on input line 798}}{29}{Experiments for Comparing Approximation Methods}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Compare the running times of the approximation methods. \relax }}{29}{figure.caption.7}}
\citation{bishop2006pattern}
\citation{hennig2012entropy}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments and Results}{30}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\citation{hennig2012entropy}
\citation{brochu2010tutorial}
\citation{hernandez2014predictive}
\newlabel{IR}{{4.1}{31}{Experiments and Results}{equation.4.0.1}{}}
\newlabel{Euclidean}{{4.2}{31}{Experiments and Results}{equation.4.0.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Within-Model Comparison}{31}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}1D Test Functions}{32}{subsection.4.1.1}}
\newlabel{1Dtestfunctions}{{\caption@xref {1Dtestfunctions}{ on input line 854}}{32}{1D Test Functions}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces  Examples of 1D within model test functions. All test function is constructed using a GP prior with a squared-exponential covariance function. The hyperparmeters of the GP model are $ l_1^2=0.01$ , $\gamma ^2=1 $, $\sigma _n^2= 10^{-6}$\relax }}{32}{figure.caption.8}}
\newlabel{1DtestIR}{{\caption@xref {1DtestIR}{ on input line 864}}{33}{1D Test Functions}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces  Median immediate regrets of ESBOPA, EI and PI in the 1D within model experiments over 50 test functions.\relax }}{33}{figure.caption.9}}
\newlabel{1DtestXnorm2}{{\caption@xref {1DtestXnorm2}{ on input line 872}}{33}{1D Test Functions}{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  Median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ of ESBOPA, EI and PI in the 1D within model experiments over 50 test functions\relax }}{33}{figure.caption.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}2D Test Functions}{34}{subsection.4.1.2}}
\newlabel{2Dtestfunctions}{{\caption@xref {2Dtestfunctions}{ on input line 882}}{34}{2D Test Functions}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Examples of 2D within model test functions which are generated using a GP prior with a squared-exponential covariance function. The hyperparmeters of the GP model are $ l_1^2=l_2^2=0.1$ , $\gamma ^2=1 $, $\sigma _n^2= 10^{-6}$. Subfigures in the top row are the 3D mesh plots of the three test functions while their corresponding contour plots are shown in the bottom row.The location of the true global minimiser for each test function is indicated with a red circle.\relax }}{34}{figure.caption.11}}
\newlabel{2DtestIR}{{\caption@xref {2DtestIR}{ on input line 891}}{35}{2D Test Functions}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces  Median immediate regrets of ESBOPA, EI and PI in the within model experiments over 2D domain.\relax }}{35}{figure.caption.12}}
\newlabel{2DtestXnorm2}{{\caption@xref {2DtestXnorm2}{ on input line 898}}{35}{2D Test Functions}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces  Median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ of ESBOPA, EI and PI in the within model experiments over 2D domain.\relax }}{35}{figure.caption.13}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Benchmark Test Functions}{36}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Branin 2D}{36}{subsection.4.2.1}}
\newlabel{2DBranin}{{\caption@xref {2DBranin}{ on input line 920}}{36}{Branin 2D}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces  The modified Branin 2D function with the globally minimum value of -0.9973 at three locations $\mathbf  {x}_{*} = [0.1239, 0.8183], [0.5428, 0.1517]$ and $ [0.9617,0.1650]$, denoted by red circles.\relax }}{36}{figure.caption.14}}
\citation{shahriari2016taking}
\newlabel{2DBraninIR}{{\caption@xref {2DBraninIR}{ on input line 929}}{37}{Branin 2D}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces  Median immediate regrets of ESBOPA, EI and PI in the Branin 2D problem.\relax }}{37}{figure.caption.15}}
\newlabel{2DBraninXnorm2}{{\caption@xref {2DBraninXnorm2}{ on input line 936}}{37}{Branin 2D}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces  Median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ of ESBOPA, EI and PI in the Branin 2D problem.\relax }}{37}{figure.caption.16}}
\newlabel{2Dbraninevaluationpoints}{{\caption@xref {2Dbraninevaluationpoints}{ on input line 944}}{38}{Branin 2D}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces  50 evaluations taken by ESBOPA, EI and PI in the Branin 2D problem. In the top row row of subfigures, the white crosses indicate the consecutive 50 evaluation samples proposed by the three algorithms. In the bottom row of subfigures, the yellow triangles indicate the 50 best guesses of the global minimiser recommended by all three algorithms after each corresponding evaluation.\relax }}{38}{figure.caption.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Rosenbrock 2D}{38}{subsection.4.2.2}}
\newlabel{2Drosenbrock}{{\caption@xref {2Drosenbrock}{ on input line 959}}{39}{Rosenbrock 2D}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The modified Rosenbrock 2D function with the globally minimum value of -10 at $\mathbf  {x}_*=[0.5, 0.5]$, denoted by the red circle.\relax }}{39}{figure.caption.18}}
\newlabel{Rosenbrock_ESBOPAV4_IR}{{\caption@xref {Rosenbrock_ESBOPAV4_IR}{ on input line 967}}{39}{Rosenbrock 2D}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces  Median immediate regrets of ESBOPA, EI and PI in the Rosenbrock 2D problem.\relax }}{39}{figure.caption.19}}
\citation{hernandez2014predictive}
\citation{hernandez2014predictive}
\newlabel{Rosenbrock_ESBOPAV4_Xnorm}{{\caption@xref {Rosenbrock_ESBOPAV4_Xnorm}{ on input line 974}}{40}{Rosenbrock 2D}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces  Median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ of ESBOPA, EI and PI in the Rosenbrock 2D problem.\relax }}{40}{figure.caption.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Hartmann 6D}{40}{subsection.4.2.3}}
\newlabel{6DHartIR}{{\caption@xref {6DHartIR}{ on input line 1007}}{41}{Hartmann 6D}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces  Median immediate regrets for ESBOPA, EI and PI in the experiments with Hartmann 6D.\relax }}{41}{figure.caption.21}}
\newlabel{6DHartXnorm}{{\caption@xref {6DHartXnorm}{ on input line 1014}}{41}{Hartmann 6D}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces  Median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ for ESBOPA, EI and PI in the experiments with Hartmann 6D.\relax }}{41}{figure.caption.22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}The Effect of Noise on Performance}{42}{section.4.3}}
\newlabel{RosenbrocknoisyIRandL2}{{\caption@xref {RosenbrocknoisyIRandL2}{ on input line 1027}}{42}{The Effect of Noise on Performance}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces  Comparison of ESBOPA, EI and PI in the Rosenbrock 2D problem with noise levels 0.001, 0.01, 0.1. Subfigures on the left column show the results of the median immediate regret. Subfigures on the right column show the results of the median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ regret.\relax }}{42}{figure.caption.23}}
\newlabel{BraninnoisyIRandL2}{{\caption@xref {BraninnoisyIRandL2}{ on input line 1068}}{43}{The Effect of Noise on Performance}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces  Comparison of ESBOPA, EI and PI in the Branin 2D problem with noise levels 0.001, 0.01, 0.1. Subfigures on the left column show the results of the median immediate regret. Subfigures on the right column show the results of the median Euclidean distance between the global minimiser $\mathbf  {x}_{*}$ and the best recommendation $ \mathaccentV {hat}05E{\mathbf  {x}}_{n}$ regret. \relax }}{43}{figure.caption.24}}
\bibstyle{plain}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{44}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{44}{chapter.5}}
\bibcite{brochu2010tutorial}{1}
\bibcite{shahriari2016taking}{2}
\bibcite{mockus1994application}{3}
\bibcite{rasmussen2006gaussian}{4}
\bibcite{bishop2006pattern}{5}
\bibcite{kushner1964new}{6}
\bibcite{jones2001taxonomy}{7}
\bibcite{dixon1978towards}{8}
\bibcite{lizotte2008practical}{9}
\bibcite{cox1992statistical}{10}
\bibcite{srinivas2009gaussian}{11}
\bibcite{hennig2012entropy}{12}
\bibcite{hernandez2014predictive}{13}
\bibcite{bochner2016lectures}{14}
\bibcite{rahimi2007random}{15}
\bibcite{snoek2012practical}{16}
\bibcite{vanhatalo2011bayesian}{17}
\bibcite{wang2017max}{18}
\bibcite{gunter2014sampling}{19}
\bibcite{huber2008entropy}{20}
